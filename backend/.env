# Frontend Configuration
APP_DOMAIN=http://localhost:3000/docs
BASE_PATH=/docs
API_DOMAIN=http://localhost:8080
IAM_DOMAIN=http://localhost:5000
CLIENT_ID=dots-ocr-app
IAM_SCOPE=openid profile email

# VLLM Server Configuration
VLLM_IP=localhost
VLLM_PORT=8001
VLLM_MODEL_NAME=dots_ocr

# Inference Configuration
TEMPERATURE=0.1
TOP_P=1.0
MAX_COMPLETION_TOKENS=52640

# Processing Configuration
NUM_THREAD=2
DPI=200
OUTPUT_DIR=./backend/output

# Image Processing Configuration
# MIN_PIXELS: Minimum number of pixels for image resizing (default: 3136)
# MAX_PIXELS: Maximum number of pixels for image resizing (default: 11289600)
# Set to None to disable resizing, or set a lower value to reduce memory usage
MIN_PIXELS=None
MAX_PIXELS=8000000

# FastAPI Server Configuration
API_HOST=0.0.0.0
API_PORT=8080

# Worker Pool Configuration
NUM_WORKERS=2

# DeepSeek OCR Service Configuration
DEEPSEEK_OCR_IP=localhost
DEEPSEEK_OCR_PORT=8005
DEEPSEEK_OCR_MODEL_NAME=deepseek-ai/DeepSeek-OCR
# Note: The model name will show as Qwen/Qwen3-0.6B even when DeepSeek-OCR is loaded
# This is expected because DeepSeek-OCR is built on top of Qwen3-0.6B base model
DEEPSEEK_OCR_TEMPERATURE=0.0
# Max tokens for output generation (model max_model_len is 8192, leave room for input)
# For large images with many crops, input can be 2000-4000 tokens
# Setting to 6144 leaves room for ~2000 input tokens
DEEPSEEK_OCR_MAX_TOKENS=6144
DEEPSEEK_OCR_NGRAM_SIZE=30
DEEPSEEK_OCR_WINDOW_SIZE=90
# Whitelist token IDs for <td> and </td> tags (comma-separated)
DEEPSEEK_OCR_WHITELIST_TOKEN_IDS=128821,128822


# Gemma3 Image Analysis (Ollama) Configuration
GEMMA_OLLAMA_BASE_URL=http://127.0.0.1:11434
GEMMA_MODEL=gemma3:27b
GEMMA_TEMPERATURE=0.8
GEMMA_TIMEOUT=120

# Qwen3 Image Analysis Backend Selection
#   QWEN_BACKEND=ollama       -> use local Ollama server (QWEN_OLLAMA_BASE_URL)
#   QWEN_BACKEND=vllm         -> use vLLM OpenAI-compatible server (QWEN_VLLM_API_BASE)
#   QWEN_BACKEND=transformers -> use HuggingFace transformers library (local GPU inference)
# QWEN_BACKEND=vllm
 QWEN_BACKEND=transformers
# QWEN_BACKEND=ollama

# Qwen3 reasoning mode
#   QWEN_REASONING_MODE=thinking    -> reasoning model that emits <think>...</think>
#   QWEN_REASONING_MODE=instruction -> instruction-only model (no reasoning blocks)
QWEN_REASONING_MODE=instruction
#QWEN_REASONING_MODE=thinking

# Qwen3 Image Analysis via vLLM / OpenAI-compatible API
# Example: Qwen2.5-VL-72B-Instruct-AWQ served by vLLM on http://localhost:8006
# QWEN_VLLM_API_BASE=http://localhost:8006/v1
# QWEN_MODEL=Qwen/Qwen2.5-VL-32B-Instruct-AWQ
# QWEN_TEMPERATURE=0.8
# QWEN_TIMEOUT=360
# QWEN_MAX_TOKENS=20480

# Qwen3 Image Analysis via Ollama (example config; use if QWEN_BACKEND=ollama)
# QWEN_OLLAMA_BASE_URL=http://127.0.0.1:11434
# QWEN_MODEL=qwen3-vl:8b
# QWEN_TEMPERATURE=0.5
# QWEN_TIMEOUT=360
# QWEN_MAX_TOKENS=40560

# Qwen3 Image Analysis via Transformers (use if QWEN_BACKEND=transformers)
# Model to load from HuggingFace (default: Qwen/Qwen3-VL-8B-Instruct)
QWEN_TRANSFORMERS_MODEL=Qwen/Qwen3-VL-8B-Instruct
# Cache directory for model downloads (default: ~/.cache/huggingface)
# QWEN_TRANSFORMERS_CACHE_DIR=~/huggingface_cache
# GPU devices to use (comma-separated, default: 4,5,6,7)
QWEN_TRANSFORMERS_GPU_DEVICES=4,5,6,7
# Data type for model weights (bfloat16, float16, float32)
QWEN_TRANSFORMERS_DTYPE=bfloat16
# Attention implementation (eager=default, flash_attention_2=faster but requires flash-attn package)
QWEN_TRANSFORMERS_ATTN_IMPL=eager
# Generation hyperparameters (VL defaults from Qwen3 documentation)
QWEN_TRANSFORMERS_MAX_NEW_TOKENS=16384
QWEN_TRANSFORMERS_TOP_P=0.8
QWEN_TRANSFORMERS_TOP_K=20
QWEN_TRANSFORMERS_REPETITION_PENALTY=1.0

# Image analysis backend for dots_ocr_service (gemma3 or qwen3)
IMAGE_ANALYSIS_BACKEND=qwen3
