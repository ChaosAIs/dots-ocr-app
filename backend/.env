# HuggingFace Cache Directory
# IMPORTANT: This must be set BEFORE Python imports transformers library.
# If using dotenv, this may not take effect early enough.
# Recommended: export HF_HOME=~/huggingface_cache in your shell or use start_with_transformers.sh
HF_HOME=~/huggingface_cache

# Frontend Configuration
APP_DOMAIN=http://localhost:3000/docs
BASE_PATH=/docs
API_DOMAIN=http://localhost:8080
IAM_DOMAIN=http://localhost:5000
CLIENT_ID=dots-ocr-app
IAM_SCOPE=openid profile email

# ------------------------------------------------------------------------------
# DOTS OCR SERVICE - vLLM Backend (Port: 8001)
# Used for: Main document/PDF/image conversion to Markdown with layout detection
# This is the primary OCR engine for page layout analysis and content extraction
# ------------------------------------------------------------------------------
DOTS_OCR_VLLM_HOST=localhost
DOTS_OCR_VLLM_PORT=8001
DOTS_OCR_VLLM_MODEL=dots_ocr

# Inference Configuration
TEMPERATURE=0.1
TOP_P=1.0
MAX_COMPLETION_TOKENS=52640

# Processing Configuration
# Number of parallel threads for PDF page processing
# Increase to 4 for faster processing of multi-page PDFs (if GPU memory allows)
# Keep at 2 if experiencing GPU memory issues
NUM_THREAD=2
# DPI for PDF to image conversion
# 150 DPI: Good balance between quality and speed (recommended for most documents)
# 200 DPI: Higher quality but slower processing
# 100 DPI: Faster but may lose detail in small text
DPI=200
OUTPUT_DIR=./backend/output

# Image Processing Configuration
# MIN_PIXELS: Minimum number of pixels for image resizing (default: 3136)
# MAX_PIXELS: Maximum number of pixels for image resizing (default: 11289600)
# Set to None to disable resizing, or set a lower value to reduce memory usage
MIN_PIXELS=None
MAX_PIXELS=8000000

# FastAPI Server Configuration
API_HOST=0.0.0.0
API_PORT=8080

# Worker Pool Configuration
# Reduced to 2 to lower GPU memory pressure and prevent timeouts
NUM_WORKERS=2

# DeepSeek OCR Service Configuration
DEEPSEEK_OCR_IP=localhost
DEEPSEEK_OCR_PORT=8005
DEEPSEEK_OCR_MODEL_NAME=deepseek-ai/DeepSeek-OCR
# Note: The model name will show as Qwen/Qwen3-0.6B even when DeepSeek-OCR is loaded
# This is expected because DeepSeek-OCR is built on top of Qwen3-0.6B base model
DEEPSEEK_OCR_TEMPERATURE=0.0
# Max tokens for output generation (model max_model_len is 8192, leave room for input)
# For large images with many crops, input can be 2000-4000 tokens
# Setting to 6144 leaves room for ~2000 input tokens
DEEPSEEK_OCR_MAX_TOKENS=6144
DEEPSEEK_OCR_NGRAM_SIZE=30
DEEPSEEK_OCR_WINDOW_SIZE=90
# Whitelist token IDs for <td> and </td> tags (comma-separated)
DEEPSEEK_OCR_WHITELIST_TOKEN_IDS=128821,128822


# ==============================================================================
# OLLAMA SERVICES CONFIGURATION
# ==============================================================================
# Two separate Ollama instances are used:
#   1. ollama-llm (port 11434)       -> LLM text inference for RAG agent
#   2. ollama-qwen3-vl (port 11435)  -> Vision-Language model for OCR
# ==============================================================================

# ------------------------------------------------------------------------------
# OCR SERVICE - Ollama Qwen3-VL (Docker: ollama-qwen3-vl, Port: 11435)
# Used for: Image OCR, document image analysis, vision tasks
# ------------------------------------------------------------------------------
OCR_OLLAMA_BASE_URL=http://127.0.0.1:11435
OCR_OLLAMA_MODEL=qwen3-vl:8b-instruct
OCR_OLLAMA_TEMPERATURE=0.5
OCR_OLLAMA_TIMEOUT=360
OCR_OLLAMA_MAX_TOKENS=40560

# Image Analysis Backend Selection (which backend to use for image OCR/analysis)
#   ollama       -> use Ollama server (OCR_OLLAMA_BASE_URL)
#   vllm         -> use vLLM OpenAI-compatible server (QWEN_VLLM_API_BASE)
#   transformers -> use HuggingFace transformers library (local GPU inference)
IMAGE_ANALYSIS_BACKEND=ollama

# OCR reasoning mode
#   thinking    -> reasoning model that emits <think>...</think>
#   instruction -> instruction-only model (no reasoning blocks)
QWEN_REASONING_MODE=instruction

# Image analysis backend for dots_ocr_service (gemma3 or qwen3)
IMAGE_ANALYSIS_BACKEND=qwen3

# Image Size Detection Threshold for OCR
# Simple images: area < threshold, Complex images: area >= threshold
QWEN_IMAGE_PIXEL_AREA_THRESHOLD=250000

# ------------------------------------------------------------------------------
# RAG AGENT LLM BACKEND SELECTION
# Used for: RAG query processing, text generation, agent responses
# Options: ollama, vllm
# ------------------------------------------------------------------------------
RAG_LLM_BACKEND=vllm

# ------------------------------------------------------------------------------
# RAG AGENT SERVICE - Ollama LLM (Docker: ollama-llm, Port: 11434)
# Active when RAG_LLM_BACKEND=ollama
# ------------------------------------------------------------------------------
RAG_OLLAMA_HOST=localhost
RAG_OLLAMA_PORT=11434
RAG_OLLAMA_MODEL=qwen2.5:latest
# Smaller, faster model for query analysis (query enhancement before vector search)
RAG_OLLAMA_QUERY_MODEL=qwen2.5:latest

# ------------------------------------------------------------------------------
# RAG AGENT SERVICE - vLLM (Docker: vllm_qwen3_14b, Port: 8004)
# Active when RAG_LLM_BACKEND=vllm
# Uses OpenAI-compatible API endpoint
# ------------------------------------------------------------------------------
RAG_VLLM_HOST=localhost
RAG_VLLM_PORT=8004
RAG_VLLM_MODEL=Qwen/Qwen3-14B
RAG_VLLM_API_KEY=EMPTY
# Reasoning mode for Qwen3 model:
#   instruction -> direct answers without reasoning blocks (default)
#   thinking    -> enable chain-of-thought reasoning with <think>...</think> blocks
RAG_VLLM_REASONING_MODE=instruction

# RAG Auto-Indexing Configuration
AUTO_INDEX_ON_STARTUP=false

# File Summary Configuration
# Enable/disable LLM-based file summary generation during indexing and query
# When disabled, file summaries will not be generated (saves LLM calls)
# and two-phase retrieval will skip file summary search
FILE_SUMMARY_ENABLED=false

# ------------------------------------------------------------------------------
# GEMMA3 IMAGE ANALYSIS (Legacy/Alternative - uses RAG Ollama server)
# ------------------------------------------------------------------------------
GEMMA_OLLAMA_BASE_URL=http://127.0.0.1:11434
GEMMA_MODEL=gemma3:27b
GEMMA_TEMPERATURE=0.8
GEMMA_TIMEOUT=120

# ------------------------------------------------------------------------------
# QWEN3 VIA VLLM (Alternative backend, if IMAGE_ANALYSIS_BACKEND=vllm)
# ------------------------------------------------------------------------------
# QWEN_VLLM_API_BASE=http://localhost:8006/v1
# QWEN_VLLM_MODEL=Qwen/Qwen2.5-VL-32B-Instruct-AWQ
# QWEN_VLLM_TEMPERATURE=0.8
# QWEN_VLLM_TIMEOUT=360
# QWEN_VLLM_MAX_TOKENS=20480

# ------------------------------------------------------------------------------
# QWEN3 VIA TRANSFORMERS (Alternative backend, if IMAGE_ANALYSIS_BACKEND=transformers)
# ------------------------------------------------------------------------------
QWEN_TRANSFORMERS_MODEL=Qwen/Qwen3-VL-8B-Instruct
QWEN_TRANSFORMERS_GPU_DEVICES=4,5,6
QWEN_TRANSFORMERS_DTYPE=bfloat16
QWEN_TRANSFORMERS_ATTN_IMPL=eager
QWEN_TRANSFORMERS_MAX_NEW_TOKENS=16384
QWEN_TRANSFORMERS_TOP_P=0.8
QWEN_TRANSFORMERS_TOP_K=20
QWEN_TRANSFORMERS_REPETITION_PENALTY=1.0
QWEN_TRANSFORMERS_DO_SAMPLE=false
QWEN_TRANSFORMERS_MAX_IMAGE_AREA=1000000

# Qdrant Vector Database Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Embedding Service Configuration
EMBEDDING_HOST=localhost
EMBEDDING_PORT=8003

# PostgreSQL Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=6400
POSTGRES_DB=dots_ocr
POSTGRES_USER=postgres
POSTGRES_PASSWORD=FyUbuntu@2025Ai

# Neo4j Graph Database Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=securepassword123

# GraphRAG Configuration
# Enable/disable GraphRAG features (set to false to use traditional RAG only)
GRAPH_RAG_ENABLED=true
# Enable/disable embeddings for graph nodes and relationships
GRAPH_RAG_EMBEDDINGS_ENABLED=true
# Default query mode: auto, local, global, hybrid, naive
# auto: LLM-based query mode detection
# local: Entity-focused queries
# global: Relationship-focused queries
# hybrid: Both entity and relationship queries
# naive: Simple vector search (fallback)
GRAPH_RAG_DEFAULT_MODE=auto
# Maximum gleaning iterations for entity extraction
GRAPH_RAG_MAX_GLEANING=3
# Entity extraction batch size
GRAPH_RAG_ENTITY_BATCH_SIZE=10
# Top-k results for graph retrieval
GRAPH_RAG_TOP_K=60
