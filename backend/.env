# HuggingFace Cache Directory
# IMPORTANT: This must be set BEFORE Python imports transformers library.
# If using dotenv, this may not take effect early enough.
# Recommended: export HF_HOME=~/huggingface_cache in your shell or use start_with_transformers.sh
HF_HOME=~/huggingface_cache

# Frontend Configuration
APP_DOMAIN=http://localhost:3000/docs
BASE_PATH=/docs
API_DOMAIN=http://localhost:8080
IAM_DOMAIN=http://localhost:5000
CLIENT_ID=dots-ocr-app
IAM_SCOPE=openid profile email

# ------------------------------------------------------------------------------
# DOTS OCR SERVICE - vLLM Backend (Port: 8001)
# Used for: Main document/PDF/image conversion to Markdown with layout detection
# This is the primary OCR engine for page layout analysis and content extraction
# ------------------------------------------------------------------------------
DOTS_OCR_VLLM_HOST=localhost
DOTS_OCR_VLLM_PORT=8001
DOTS_OCR_VLLM_MODEL=dots_ocr

# Inference Configuration
TEMPERATURE=0.1
TOP_P=1.0
MAX_COMPLETION_TOKENS=52640

# Processing Configuration
# Number of parallel threads for PDF page processing
# Increase to 4 for faster processing of multi-page PDFs (if GPU memory allows)
# Keep at 2 if experiencing GPU memory issues
NUM_THREAD=2
# DPI for PDF to image conversion
# 150 DPI: Good balance between quality and speed (recommended for most documents)
# 200 DPI: Higher quality but slower processing
# 100 DPI: Faster but may lose detail in small text
DPI=200
INPUT_DIR=./input
OUTPUT_DIR=./output

# Image Processing Configuration
# MIN_PIXELS: Minimum number of pixels for image resizing (default: 3136)
# MAX_PIXELS: Maximum number of pixels for image resizing (default: 11289600)
# Set to None to disable resizing, or set a lower value to reduce memory usage
MIN_PIXELS=None
MAX_PIXELS=8000000

# FastAPI Server Configuration
API_HOST=0.0.0.0
API_PORT=8080

# Worker Pool Configuration
# Reduced to 2 to lower GPU memory pressure and prevent timeouts
NUM_WORKERS=2

# ==============================================================================
# HIERARCHICAL TASK QUEUE SYSTEM CONFIGURATION
# ==============================================================================
# The task queue uses a 3-level hierarchy: Document → Page → Chunk
# Each level has 3 processing phases: OCR → Vector Index → GraphRAG Index
#
# Enable/disable the task queue system for automatic OCR and indexing
# When enabled, files are automatically processed on upload without manual triggers
TASK_QUEUE_ENABLED=true

# Interval (in seconds) for periodic queue maintenance (stale task detection, orphan cleanup)
# Default: 60 seconds (1 minute)
TASK_QUEUE_CHECK_INTERVAL=60

# Worker poll interval (in seconds) - how often workers check for new tasks
# Lower values = faster pickup but more database queries
# Default: 5 seconds
WORKER_POLL_INTERVAL=5

# Heartbeat Configuration
# Workers update heartbeat timestamp periodically while processing tasks.
# If a task's heartbeat is not updated within the stale timeout, it's considered dead.
#
# Heartbeat interval (in seconds) - how often workers update their heartbeat
# Should be less than STALE_TASK_TIMEOUT / 2 to allow at least 2 updates before timeout
# Default: 30 seconds
HEARTBEAT_INTERVAL=30

# Stale task timeout (in seconds) - tasks with no heartbeat for this long are released
# Formula: Should be > 2 * HEARTBEAT_INTERVAL to tolerate missed heartbeats
# For long-running tasks (GraphRAG), this determines recovery time after worker death
# Default: 300 seconds (5 minutes) - allows 10 missed heartbeats before declaring dead
STALE_TASK_TIMEOUT=300

# Max retries for failed tasks at each level
# After this many failures, task is marked as permanently failed
# Default: 3 retries
MAX_TASK_RETRIES=3

# DeepSeek OCR Service Configuration
DEEPSEEK_OCR_IP=localhost
DEEPSEEK_OCR_PORT=8005
DEEPSEEK_OCR_MODEL_NAME=deepseek-ai/DeepSeek-OCR
# Note: The model name will show as Qwen/Qwen3-0.6B even when DeepSeek-OCR is loaded
# This is expected because DeepSeek-OCR is built on top of Qwen3-0.6B base model
DEEPSEEK_OCR_TEMPERATURE=0.0
# Max tokens for output generation (model max_model_len is 8192, leave room for input)
# For large images with many crops, input can be 2000-4000 tokens
# Setting to 6144 leaves room for ~2000 input tokens
DEEPSEEK_OCR_MAX_TOKENS=6144
DEEPSEEK_OCR_NGRAM_SIZE=30
DEEPSEEK_OCR_WINDOW_SIZE=90
# Whitelist token IDs for <td> and </td> tags (comma-separated)
DEEPSEEK_OCR_WHITELIST_TOKEN_IDS=128821,128822


# ==============================================================================
# OLLAMA SERVICES CONFIGURATION
# ==============================================================================
# Two separate Ollama instances are used:
#   1. ollama-llm (port 11434)       -> LLM text inference for RAG agent
#   2. ollama-qwen3-vl (port 11435)  -> Vision-Language model for OCR
# ==============================================================================

# ------------------------------------------------------------------------------
# OCR SERVICE - Ollama Qwen3-VL (Docker: ollama-qwen3-vl, Port: 11435)
# Used for: Image OCR, document image analysis, vision tasks
# ------------------------------------------------------------------------------
OCR_OLLAMA_BASE_URL=http://127.0.0.1:11435
OCR_OLLAMA_MODEL=qwen3-vl:8b-instruct
OCR_OLLAMA_TEMPERATURE=0.5
OCR_OLLAMA_TIMEOUT=360
OCR_OLLAMA_MAX_TOKENS=40560

# Image Analysis Backend Selection (which backend to use for image OCR/analysis)
#   ollama       -> use Ollama server (OCR_OLLAMA_BASE_URL)
#   vllm         -> use vLLM OpenAI-compatible server (QWEN_VLLM_API_BASE)
#   transformers -> use HuggingFace transformers library (local GPU inference)
IMAGE_ANALYSIS_BACKEND=ollama

# OCR reasoning mode
#   thinking    -> reasoning model that emits <think>...</think>
#   instruction -> instruction-only model (no reasoning blocks)
QWEN_REASONING_MODE=instruction

# Image analysis backend for dots_ocr_service (gemma3 or qwen3)
IMAGE_ANALYSIS_BACKEND=qwen3

# Image Size Detection Threshold for OCR
# Simple images: area < threshold, Complex images: area >= threshold
QWEN_IMAGE_PIXEL_AREA_THRESHOLD=250000

# ------------------------------------------------------------------------------
# RAG AGENT LLM BACKEND SELECTION
# Used for: RAG query processing, text generation, agent responses
# Options: ollama, vllm
# ------------------------------------------------------------------------------
RAG_LLM_BACKEND=vllm

# ------------------------------------------------------------------------------
# RAG AGENT SERVICE - Ollama LLM (Docker: ollama-llm, Port: 11434)
# Active when RAG_LLM_BACKEND=ollama
# ------------------------------------------------------------------------------
RAG_OLLAMA_HOST=localhost
RAG_OLLAMA_PORT=11434
RAG_OLLAMA_MODEL=qwen2.5:latest
# Smaller, faster model for query analysis (query enhancement before vector search)
RAG_OLLAMA_QUERY_MODEL=qwen2.5:latest

# ------------------------------------------------------------------------------
# RAG AGENT SERVICE - vLLM (Docker: vllm_qwen3_14b, Port: 8004)
# Active when RAG_LLM_BACKEND=vllm
# Uses OpenAI-compatible API endpoint
# ------------------------------------------------------------------------------
RAG_VLLM_HOST=localhost
RAG_VLLM_PORT=8004
RAG_VLLM_MODEL=Qwen/Qwen3-8B
RAG_VLLM_API_KEY=EMPTY
# Reasoning mode for Qwen3 model:
#   instruction -> direct answers without reasoning blocks (default)
#   thinking    -> enable chain-of-thought reasoning with <think>...</think> blocks
RAG_VLLM_REASONING_MODE=instruction

# Auto-Recovery Configuration
# AUTO_RESUME_OCR: Resume incomplete OCR conversion tasks on startup (default: true)
# This will automatically resume OCR conversion for documents with pending/converting/partial/failed status
AUTO_RESUME_OCR=true
# AUTO_RESUME_INDEXING: Resume incomplete indexing tasks on startup (default: false)
# This will automatically resume indexing for documents with pending/failed status
# Set to false to prevent any automatic indexing on startup
AUTO_RESUME_INDEXING=true

# File Summary Configuration
# Enable/disable LLM-based file summary generation during indexing and query
# When disabled, file summaries will not be generated (saves LLM calls)
# and two-phase retrieval will skip file summary search
FILE_SUMMARY_ENABLED=false

# ------------------------------------------------------------------------------
# GEMMA3 IMAGE ANALYSIS (Legacy/Alternative - uses RAG Ollama server)
# ------------------------------------------------------------------------------
GEMMA_OLLAMA_BASE_URL=http://127.0.0.1:11434
GEMMA_MODEL=gemma3:27b
GEMMA_TEMPERATURE=0.8
GEMMA_TIMEOUT=120

# ------------------------------------------------------------------------------
# QWEN3 VIA VLLM (Alternative backend, if IMAGE_ANALYSIS_BACKEND=vllm)
# ------------------------------------------------------------------------------
# QWEN_VLLM_API_BASE=http://localhost:8006/v1
# QWEN_VLLM_MODEL=Qwen/Qwen2.5-VL-32B-Instruct-AWQ
# QWEN_VLLM_TEMPERATURE=0.8
# QWEN_VLLM_TIMEOUT=360
# QWEN_VLLM_MAX_TOKENS=20480

# ------------------------------------------------------------------------------
# QWEN3 VIA TRANSFORMERS (Alternative backend, if IMAGE_ANALYSIS_BACKEND=transformers)
# ------------------------------------------------------------------------------
QWEN_TRANSFORMERS_MODEL=Qwen/Qwen3-VL-8B-Instruct
QWEN_TRANSFORMERS_GPU_DEVICES=4,5,6
QWEN_TRANSFORMERS_DTYPE=bfloat16
QWEN_TRANSFORMERS_ATTN_IMPL=eager
QWEN_TRANSFORMERS_MAX_NEW_TOKENS=16384
QWEN_TRANSFORMERS_TOP_P=0.8
QWEN_TRANSFORMERS_TOP_K=20
QWEN_TRANSFORMERS_REPETITION_PENALTY=1.0
QWEN_TRANSFORMERS_DO_SAMPLE=false
QWEN_TRANSFORMERS_MAX_IMAGE_AREA=1000000

# Qdrant Vector Database Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Embedding Service Configuration
EMBEDDING_HOST=localhost
EMBEDDING_PORT=8003

# PostgreSQL Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=6400
POSTGRES_DB=dots_ocr
POSTGRES_USER=postgres
POSTGRES_PASSWORD=FyUbuntu@2025Ai

# JWT Authentication Configuration
JWT_SECRET_KEY=your-secret-key-change-in-production-to-random-string-abc123xyz789
# Set very long expiration so session only ends when user clicks logout
ACCESS_TOKEN_EXPIRE_MINUTES=525600
REFRESH_TOKEN_EXPIRE_DAYS=3650

# Neo4j Graph Database Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=securepassword123

# GraphRAG Configuration (Following Graph-R1 paper design)
# Enable/disable GraphRAG features (set to false to use traditional RAG only)
GRAPH_RAG_INDEX_ENABLED=true
GRAPH_RAG_QUERY_ENABLED=true
# Enable/disable embeddings for graph nodes and relationships
GRAPH_RAG_EMBEDDINGS_ENABLED=true
# Default query mode: auto, local, global, hybrid
# auto: LLM-based query mode detection (recommended)
# local: Entity-focused queries
# global: Relationship-focused queries
# hybrid: Combined entity and relationship queries (default)
GRAPH_RAG_DEFAULT_MODE=auto

# ========== Performance Optimization Settings ==========
# Maximum gleaning iterations for entity extraction
# 0 = No gleaning (fastest, 75% fewer LLM calls)
# 1 = One gleaning pass (balanced, 50% fewer LLM calls)
# 2-3 = Multiple passes (thorough but slow)
# Recommendation: 0 for speed, 1 for balanced quality/speed
GRAPH_RAG_MAX_GLEANING=0

# Minimum importance score for entity extraction (1-100)
# Only entities with score >= this threshold will be stored
# 80-100: Only critical entities (very selective)
# 60-79: Important entities (recommended - good balance)
# 40-59: Moderate entities (more comprehensive)
# Below 40: All entities (not recommended - too noisy)
GRAPH_RAG_MIN_ENTITY_SCORE=40

# Entity extraction batch size
GRAPH_RAG_ENTITY_BATCH_SIZE=10
# Top-k results for graph retrieval
GRAPH_RAG_TOP_K=60
# Enable/disable Qdrant vector search (always combines with graph results)
GRAPH_RAG_VECTOR_SEARCH_ENABLED=false
# Iterative reasoning max steps (Graph-R1 think-query-retrieve-rethink cycle)
# This sets the MAXIMUM allowed steps. The LLM will analyze query complexity
# and determine the actual steps needed (1 to GRAPH_RAG_MAX_STEPS).
# Set to 1 for single-step retrieval only
# Set to 3-5 for allowing iterative reasoning on complex queries (recommended: 3)
GRAPH_RAG_MAX_STEPS=3

# Number of chunks to show per source document in GraphRAG reasoning prompts
# Higher values provide more context but increase token usage
# Recommendation: 2-3 chunks per source for balanced coverage
GRAPH_RAG_CHUNKS_PER_SOURCE=3

# Graph-R1 Reward-Based Termination (Quality Control)
# Enable/disable reward scoring for answer quality assessment
GRAPH_RAG_ENABLE_REWARD_SCORING=true
# Use LLM-based answer scoring (true) or heuristic-based (false)
# LLM scoring is more accurate but slower; heuristic is faster but less intelligent
GRAPH_RAG_USE_LLM_ANSWER_SCORING=true
# High format score threshold (0.0-1.0) - terminate if answer is well-formatted and high quality
GRAPH_RAG_FORMAT_REWARD_HIGH_THRESHOLD=0.8
# Low format score threshold (0.0-1.0) - terminate if likely hallucination
GRAPH_RAG_FORMAT_REWARD_LOW_THRESHOLD=0.3
# Minimum answer quality score (0.0-1.0) - continue if answer quality is below this
GRAPH_RAG_ANSWER_REWARD_THRESHOLD=0.6

# ==============================================================================
# ITERATIVE REASONING CONFIGURATION
# ==============================================================================
# Enable/disable iterative reasoning for vector-only search (works with or without GraphRAG)
# When enabled, queries are refined iteratively based on retrieved results
ITERATIVE_REASONING_ENABLED=false
# Default max steps for iterative reasoning (can be overridden by LLM complexity analysis)
ITERATIVE_REASONING_DEFAULT_MAX_STEPS=3
# Number of chunks to show per source in the reasoning prompt
ITERATIVE_REASONING_CHUNKS_PER_SOURCE=4

# ------------------------------------------------------------------------------
# ITERATIVE REASONING PERFORMANCE OPTIMIZATION
# ------------------------------------------------------------------------------
# These settings optimize the reasoning loop for faster response times
# by skipping unnecessary LLM THINKING calls in certain scenarios

# Skip THINKING if retrieved chunk count >= this threshold (Option 1: single-doc optimization)
# When querying a single document with sufficient chunks, skip the THINKING LLM call
# Set higher for more conservative skipping, lower for more aggressive optimization
# Default: 15 chunks (if single-doc query has 15+ chunks, proceed directly to answer)
SKIP_THINKING_CHUNK_THRESHOLD=15

# Early termination if top chunk relevance score >= this threshold (Option 7)
# When the best matching chunk has high relevance, skip THINKING and generate answer
# Range: 0.0-1.0 (higher = stricter, lower = more aggressive skipping)
# Default: 0.6 (60% similarity triggers early termination if 10+ chunks retrieved)
EARLY_TERMINATION_SCORE_THRESHOLD=0.6

# Number of top chunks to send to THINKING LLM (Option 2: reduce context size)
# Instead of sending all chunks, only send the top N highest-scoring chunks
# This reduces token usage and speeds up the THINKING decision
# Default: 10 (send top 10 chunks instead of all 20)
THINKING_TOP_CHUNKS_COUNT=10

# Skip vector search for follow-up queries when GraphRAG is enabled (Option A)
# When true, follow-up queries (step 2+) use GraphRAG-only retrieval, skipping vector search
# This avoids redundant results since initial vector search already found similar chunks
# GraphRAG's entity/relationship traversal is better suited for refined follow-up queries
# Only applies when GraphRAG is enabled; if disabled, vector search is always used
# Default: true (skip vector search for follow-ups when GraphRAG available)
FOLLOWUP_SKIP_VECTOR_SEARCH=true

# ==============================================================================
# ADAPTIVE CHUNKING CONFIGURATION
# ==============================================================================
# Enable/disable adaptive domain-aware chunking (default: true for optimal chunking)
# When enabled, documents are classified before chunking to select optimal strategy
# This provides domain-aware chunking with rich metadata for better retrieval
ADAPTIVE_CHUNKING_ENABLED=true
# Enable LLM-based document classification (more accurate but slower)
# When disabled (recommended), uses fast rule-based classification only
# Rule-based classification is sufficient for most use cases and avoids LLM latency/cost
ADAPTIVE_CHUNKING_USE_LLM=false

# ==============================================================================
# LLM-DRIVEN CHUNKING V3.0 CONFIGURATION
# ==============================================================================
# V3.0 replaces pattern-based domain classification with LLM structure analysis.
# Benefits: Eliminates fragile regex patterns, 1 LLM call per file, handles any document type.
#
# Enable LLM-driven structure analysis for chunking strategy selection (default: false)
# When enabled:
#   - Uses single LLM call per uploaded file to analyze document structure
#   - Selects from 8 predefined strategies: header_based, paragraph_based, sentence_based,
#     list_item_based, table_row_based, citation_based, log_entry_based, clause_based
#   - Supports multi-page OCR (page-based sampling) and single-file conversion (char-based)
# When disabled:
#   - Uses V2.0 pattern-based domain classification (existing behavior)
LLM_DRIVEN_CHUNKING_ENABLED=true

# Tabular skip analysis (V2.0 feature, works with both V2.0 and V3.0)
# When strategy is "table_preserving", analyze whether to skip row-level chunking
# for pure tabular documents (high table ratio), generating summary chunks instead
TABULAR_SKIP_ENABLED=true
# Threshold for tabular skip decision (0.0-1.0)
# Documents scoring >= threshold will skip row chunking and use summary chunks
TABULAR_SKIP_THRESHOLD=0.6

# Default chunk sizes per domain (used when adaptive chunking is enabled)
# Legal documents: contracts, agreements, terms of service
CHUNKING_LEGAL_SIZE=1024
CHUNKING_LEGAL_OVERLAP_PERCENT=5
# Academic documents: research papers, theses, articles
CHUNKING_ACADEMIC_SIZE=768
CHUNKING_ACADEMIC_OVERLAP_PERCENT=15
# Medical documents: clinical notes, records, reports
CHUNKING_MEDICAL_SIZE=1024
CHUNKING_MEDICAL_OVERLAP_PERCENT=5
# Engineering documents: specs, requirements, API docs
CHUNKING_ENGINEERING_SIZE=1024
CHUNKING_ENGINEERING_OVERLAP_PERCENT=5
# Education documents: textbooks, course materials, exams
CHUNKING_EDUCATION_SIZE=768
CHUNKING_EDUCATION_OVERLAP_PERCENT=10
# Financial documents: receipts, invoices, statements
CHUNKING_FINANCIAL_SIZE=1024
CHUNKING_FINANCIAL_OVERLAP_PERCENT=15
# General documents: default for unclassified
CHUNKING_GENERAL_SIZE=512
CHUNKING_GENERAL_OVERLAP_PERCENT=10

# Size thresholds for document classification
# Documents below this token count may be kept whole
CHUNKING_SMALL_DOC_TOKENS=1000
# Documents above this token count use more aggressive splitting
CHUNKING_LARGE_DOC_TOKENS=10000

# ==============================================================================
# PARENT/CHILD CHUNK CONFIGURATION
# ==============================================================================
# Enable parent/child chunk design for hierarchical context retrieval
# When enabled, large sections create both a parent chunk (full section)
# and child chunks (split pieces), with bidirectional references
PARENT_CHUNK_ENABLED=true
# Automatically include parent chunks when child chunks match a query
# This provides expanded context for better understanding
# When true: Parent chunks are always fetched for child matches
# When false: Only child chunks are returned (caller must fetch parents manually)
PARENT_CHUNK_AUTO_INCLUDE=true

# ==============================================================================
# PARENT CHUNK SUMMARIZATION CONFIGURATION
# ==============================================================================
# Enable LLM-based summarization for large parent chunks during chunking
# When enabled, parent chunks exceeding the token threshold will get a summary
# stored in metadata, which is used during retrieval to reduce context size
GENERATE_PARENT_SUMMARIES=true
# Token threshold to trigger parent chunk summarization
# Parent chunks with more tokens than this will be summarized
PARENT_SUMMARY_TOKEN_THRESHOLD=1500
# Enable/disable the summarization service (runtime toggle)
# Set to false to skip LLM summarization during retrieval (improves performance)
PARENT_CHUNK_SUMMARY_ENABLED=false
# Maximum tokens for generated parent summaries
PARENT_CHUNK_SUMMARY_MAX_TOKENS=500
# LRU cache size for parent chunk summaries (prevents redundant LLM calls)
PARENT_CHUNK_CACHE_SIZE=1000

# ==============================================================================
# TOKEN-AWARE CONTEXT BUILDING CONFIGURATION
# ==============================================================================
# Maximum total tokens for context passed to LLM during retrieval
# This ensures context fits within model's context window
CONTEXT_MAX_TOKENS=6000
# Budget allocation percentages (must sum to 100 or less)
# Percentage of token budget allocated to regular search result chunks
CONTEXT_CHUNK_BUDGET_PERCENT=60
# Percentage of token budget allocated to parent chunks (context expansion)
CONTEXT_PARENT_BUDGET_PERCENT=25
# Percentage of token budget allocated to GraphRAG context
CONTEXT_GRAPH_BUDGET_PERCENT=15
# Enable summarization for chunks exceeding their allocated budget
# When true, large chunks are summarized to fit; when false, they are truncated
CONTEXT_SUMMARIZE_LARGE_CHUNKS=true

# ==============================================================================
# QUERY RELEVANCE SCORING CONFIGURATION
# ==============================================================================
# Enable query-specific relevance scoring for all chunks
# When enabled, chunks are scored against the query using cosine similarity
# This provides dynamic, query-aware prioritization instead of static importance scores
CONTEXT_ENABLE_QUERY_SCORING=true
# Score threshold for CRITICAL priority (highest relevance, always included)
CONTEXT_SCORE_THRESHOLD_CRITICAL=0.85
# Score threshold for HIGH priority (very relevant, included if space allows)
CONTEXT_SCORE_THRESHOLD_HIGH=0.70
# Score threshold for MEDIUM priority (somewhat relevant)
CONTEXT_SCORE_THRESHOLD_MEDIUM=0.50
# Chunks below MEDIUM threshold get LOW priority and may be excluded

# ==============================================================================
# CHAT CONTEXT MANAGEMENT CONFIGURATION
# ==============================================================================
# Chat Context Configuration for short-term memory and context awareness
# Number of messages to keep in short-term memory (conversation context window)
CHAT_CONTEXT_WINDOW_SIZE=20
# Maximum input token limit for chat history passed to LLM
# When chat history exceeds this limit, only the most recent messages are kept
# Default: 4096 tokens (adjust based on your model's context window)
MAX_INPUT_TOKEN_LIMIT=4096
# Enable/disable context analysis (pronoun detection, entity extraction, reference resolution)
CHAT_ENABLE_CONTEXT_ANALYSIS=true
# Enable/disable metadata tracking (track entities, topics, documents in session metadata)
CHAT_ENABLE_METADATA_TRACKING=true
# Maximum number of entities to track per type (documents, people, topics, etc.)
CHAT_MAX_ENTITIES_PER_TYPE=10
# Maximum number of topics to track in conversation
CHAT_MAX_TOPICS=20
# Enable automatic fallback from CONVERSATION_ONLY to document search
# When enabled, if LLM response from history indicates "no data found",
# the system will automatically trigger a document search with enhanced query
CHAT_AUTO_FALLBACK_ENABLED=true

# Context Analyzer LLM Mode
# Enable LLM-based context analysis for intelligent topic/entity extraction
# When enabled (true): Uses LLM to analyze conversation context, extract topics,
#   entities (documents, people, dates), resolve pronouns, and understand intent
# When disabled (false): Falls back to fast heuristic-based pattern matching
# LLM mode provides better accuracy but adds ~200-500ms latency per message
CONTEXT_ANALYZER_USE_LLM=true

# ==============================================================================
# DOCUMENT METADATA VECTOR ROUTING CONFIGURATION
# ==============================================================================
# Enable/disable vector-based document routing (faster than LLM scoring)
# When enabled, queries are routed to relevant documents using semantic similarity
# on document metadata embeddings stored in Qdrant
METADATA_VECTOR_ROUTING_ENABLED=true

# Number of top documents to return from metadata vector search
METADATA_SEARCH_TOP_K=15

# Minimum similarity score threshold for metadata vector search (0.0-1.0)
# Lower values = more results (better recall), higher values = fewer but more relevant results
# Note: Set lower (0.3) to include documents with mixed-language content (e.g., Chinese+English)
METADATA_SCORE_THRESHOLD=0.3

# Minimum number of results from vector search before falling back to LLM scoring
# If vector search returns fewer results than this, LLM fallback is NOT triggered
# (vector results are still used if any are found)
METADATA_FALLBACK_MIN_RESULTS=3

# Adaptive relative threshold for document routing (0.0-1.0)
# Results must score >= (top_score * this_value) to be included
# Higher = stricter filtering (fewer documents), Lower = more permissive
# Example: 0.5 means results must be at least 50% of the top score
# Note: Set to 0.5 for "list all" type queries to include more similar documents
METADATA_RELATIVE_THRESHOLD=0.5

# Entity matching boost for document routing
# Boosts score when query entities match document source/content
ENTITY_MATCH_BOOST=0.3

# ==============================================================================
# REDIS CACHE CONFIGURATION
# ==============================================================================
# Redis server for analytics session caching and real-time updates
# Using existing n8n-redis container on shared-net docker network
REDIS_HOST=n8n-redis-1
REDIS_PORT=6379
REDIS_DB=1
REDIS_PASSWORD=
REDIS_SSL=false

# Analytics Session Cache Settings
# TTL for cached session state (default: 24 hours)
REDIS_SESSION_TTL=86400
REDIS_SESSION_PREFIX=analytics:session:

# Real-time Updates (pub/sub channel for WebSocket broadcasts)
REDIS_PUBSUB_CHANNEL=analytics:updates

# ==============================================================================
# TABULAR DATA OPTIMIZED WORKFLOW CONFIGURATION
# ==============================================================================
# Optimized processing path for dataset-style documents (CSV, Excel, invoices, etc.)
# This workflow:
#   - Skips row-level chunking (no embedding of individual data rows)
#   - Creates 1-3 LLM-driven summary chunks for document discovery
#   - Extracts structured data to PostgreSQL for SQL queries
#   - Uses two-stage query routing: vector search → SQL analytics

# Enable/disable the tabular data optimized workflow (default: true)
# When enabled, tabular documents use the optimized summary-only indexing path
# When disabled, all documents use standard row-level chunking
TABULAR_DATA_WORKFLOW_ENABLED=true

# Enable LLM-driven summary generation for tabular documents (default: true)
# When enabled, LLM generates semantic summaries, schema descriptions, and business context
# When disabled, uses rule-based fallback summaries (faster but less adaptive)
TABULAR_LLM_SUMMARY_ENABLED=true

# Maximum rows to analyze for summary generation (default: 10)
# More rows provide better context but increase LLM token usage
TABULAR_SUMMARY_SAMPLE_ROWS=10

# Document types to process as tabular data (comma-separated)
# These are detected from document metadata or file extensions
TABULAR_DOCUMENT_TYPES=spreadsheet,invoice,receipt,bank_statement,credit_card_statement,expense_report,inventory_report,purchase_order,sales_order,payroll,financial_statement,price_list,data_export,transaction_log,shipping_manifest

# File extensions to process as tabular data (comma-separated)
# These bypass content analysis and go directly to tabular pathway
TABULAR_FILE_EXTENSIONS=.csv,.tsv,.xlsx,.xls,.xlsm,.xlsb,.ods

# ==============================================================================
# DATA EXTRACTION CONFIGURATION
# ==============================================================================
# Enable/disable automatic structured data extraction from documents
# When enabled, eligible documents are extracted after GraphRAG indexing completes
DATA_EXTRACTION_ENABLED=true

# Extraction strategy thresholds (row counts)
# Direct LLM: Single LLM call for small tables
EXTRACTION_DIRECT_LLM_MAX_ROWS=50
# Chunked LLM: Parallel chunked extraction for medium tables
EXTRACTION_CHUNKED_LLM_MAX_ROWS=500
# Hybrid: LLM for header mapping, rules for data
EXTRACTION_HYBRID_MAX_ROWS=5000
# Above HYBRID threshold uses pure parsing

# Minimum confidence score for extracted data validation
EXTRACTION_MIN_CONFIDENCE=0.85

# ==============================================================================
# ANALYTICS SESSION CONFIGURATION
# ==============================================================================
# Enable conversational analytics features
ANALYTICS_ENABLED=true

# Session expiration (default: 24 hours)
ANALYTICS_SESSION_EXPIRY_HOURS=24

# Maximum plan iterations before requiring user confirmation
ANALYTICS_MAX_PLAN_ITERATIONS=5

# Enable intent auto-classification
ANALYTICS_AUTO_CLASSIFY_INTENT=true

# ==============================================================================
# DYNAMIC SQL GENERATION CONFIGURATION
# ==============================================================================
# Enable LLM-based dynamic SQL generation for analytics queries
# When enabled, uses multi-round LLM analysis for better field mapping:
#   - Round 1: Query intent and grouping order analysis
#   - Round 2: Field mapping to match user terms to actual schema fields
#   - Round 3: SQL generation with verified parameters
# When disabled, uses fast heuristic-based SQL generation (faster but less flexible)
ANALYTICS_USE_LLM=true

# Minimum confidence score for analytics query routing
ANALYTICS_MIN_CONFIDENCE=0.6

# ==============================================================================
# QUERY CACHE CONFIGURATION
# ==============================================================================
# Semantic caching for Q&A responses to improve response time and reduce LLM costs
# Cache entries are stored in Qdrant with workspace isolation and permission validation

# Enable/disable the query cache system (default: true)
QUERY_CACHE_ENABLED=true

# Enable pre-cache analysis using LLM (default: true)
# Analyzes questions for: dissatisfaction, self-containment, enhancement, cache worthiness
QUERY_CACHE_PRE_ANALYSIS_ENABLED=true

# Similarity thresholds by query type (0.0-1.0, higher = stricter matching)
# document_search: Policy/procedure questions - high threshold for precise matches
QUERY_CACHE_THRESHOLD_DOC_SEARCH=0.92
# data_analytics: Data queries - medium threshold, data changes frequently
QUERY_CACHE_THRESHOLD_DATA_ANALYTICS=0.90
# hybrid: Mixed queries - balanced threshold
QUERY_CACHE_THRESHOLD_HYBRID=0.88
# general: Broad questions - lower threshold for wider matching
QUERY_CACHE_THRESHOLD_GENERAL=0.85
# default: Fallback threshold
QUERY_CACHE_THRESHOLD_DEFAULT=0.90

# TTL (Time-To-Live) settings by query type (in seconds)
# document_search: 7 days - policy documents change infrequently
QUERY_CACHE_TTL_DOC_SEARCH=604800
# data_analytics: 1 hour - data changes frequently
QUERY_CACHE_TTL_DATA_ANALYTICS=3600
# hybrid: 24 hours - balanced freshness
QUERY_CACHE_TTL_HYBRID=86400
# general: 30 days - general knowledge is stable
QUERY_CACHE_TTL_GENERAL=2592000
# default: 24 hours
QUERY_CACHE_TTL_DEFAULT=86400

# Cache size limits
# Maximum cache entries per workspace
QUERY_CACHE_MAX_ENTRIES=10000
# Maximum answer length to cache (characters)
QUERY_CACHE_MAX_ANSWER_LENGTH=50000

# Search settings
# Number of candidate entries to check during cache lookup
QUERY_CACHE_SEARCH_LIMIT=5

# Cleanup settings
QUERY_CACHE_CLEANUP_BATCH_SIZE=1000
QUERY_CACHE_CLEANUP_INTERVAL_HOURS=6

# Confidence scoring for cache entries
# Initial confidence score for new entries
QUERY_CACHE_INITIAL_CONFIDENCE=0.95
# Minimum confidence before entry is invalidated
QUERY_CACHE_MIN_CONFIDENCE=0.5
# Confidence decay when user expresses dissatisfaction
QUERY_CACHE_CONFIDENCE_DECAY=0.25
# Maximum negative feedback count before invalidation
QUERY_CACHE_MAX_NEGATIVE_FEEDBACK=3

# Response Quality Evaluation (before caching)
# Enable LLM-based evaluation to determine if responses are worth caching (default: true)
# When enabled, "no information found" type responses are NOT cached
QUERY_CACHE_RESPONSE_EVALUATION_ENABLED=true
# Minimum response length to consider for caching (characters)
# Responses shorter than this are not cached
QUERY_CACHE_MIN_RESPONSE_LENGTH=50

# Qdrant collection settings
QUERY_CACHE_COLLECTION_PREFIX=query_cache_
QUERY_CACHE_EMBEDDING_DIM=2560
