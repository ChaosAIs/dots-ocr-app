# HuggingFace Cache Directory
# IMPORTANT: This must be set BEFORE Python imports transformers library.
# If using dotenv, this may not take effect early enough.
# Recommended: export HF_HOME=~/huggingface_cache in your shell or use start_with_transformers.sh
HF_HOME=~/huggingface_cache

# Frontend Configuration
APP_DOMAIN=http://localhost:3000/docs
BASE_PATH=/docs
API_DOMAIN=http://localhost:8080
IAM_DOMAIN=http://localhost:5000
CLIENT_ID=dots-ocr-app
IAM_SCOPE=openid profile email

# ------------------------------------------------------------------------------
# DOTS OCR SERVICE - vLLM Backend (Port: 8001)
# Used for: Main document/PDF/image conversion to Markdown with layout detection
# This is the primary OCR engine for page layout analysis and content extraction
# ------------------------------------------------------------------------------
DOTS_OCR_VLLM_HOST=localhost
DOTS_OCR_VLLM_PORT=8001
DOTS_OCR_VLLM_MODEL=dots_ocr

# Inference Configuration
TEMPERATURE=0.1
TOP_P=1.0
MAX_COMPLETION_TOKENS=52640

# Processing Configuration
# Number of parallel threads for PDF page processing
# Increase to 4 for faster processing of multi-page PDFs (if GPU memory allows)
# Keep at 2 if experiencing GPU memory issues
NUM_THREAD=2
# DPI for PDF to image conversion
# 150 DPI: Good balance between quality and speed (recommended for most documents)
# 200 DPI: Higher quality but slower processing
# 100 DPI: Faster but may lose detail in small text
DPI=200
OUTPUT_DIR=./backend/output

# Image Processing Configuration
# MIN_PIXELS: Minimum number of pixels for image resizing (default: 3136)
# MAX_PIXELS: Maximum number of pixels for image resizing (default: 11289600)
# Set to None to disable resizing, or set a lower value to reduce memory usage
MIN_PIXELS=None
MAX_PIXELS=8000000

# FastAPI Server Configuration
API_HOST=0.0.0.0
API_PORT=8080

# Worker Pool Configuration
# Reduced to 2 to lower GPU memory pressure and prevent timeouts
NUM_WORKERS=2

# ==============================================================================
# HIERARCHICAL TASK QUEUE SYSTEM CONFIGURATION
# ==============================================================================
# The task queue uses a 3-level hierarchy: Document → Page → Chunk
# Each level has 3 processing phases: OCR → Vector Index → GraphRAG Index
#
# Enable/disable the task queue system for automatic OCR and indexing
# When enabled, files are automatically processed on upload without manual triggers
TASK_QUEUE_ENABLED=true

# Interval (in seconds) for periodic queue maintenance (stale task detection, orphan cleanup)
# Default: 60 seconds (1 minute)
TASK_QUEUE_CHECK_INTERVAL=60

# Worker poll interval (in seconds) - how often workers check for new tasks
# Lower values = faster pickup but more database queries
# Default: 5 seconds
WORKER_POLL_INTERVAL=5

# Heartbeat Configuration
# Workers update heartbeat timestamp periodically while processing tasks.
# If a task's heartbeat is not updated within the stale timeout, it's considered dead.
#
# Heartbeat interval (in seconds) - how often workers update their heartbeat
# Should be less than STALE_TASK_TIMEOUT / 2 to allow at least 2 updates before timeout
# Default: 30 seconds
HEARTBEAT_INTERVAL=30

# Stale task timeout (in seconds) - tasks with no heartbeat for this long are released
# Formula: Should be > 2 * HEARTBEAT_INTERVAL to tolerate missed heartbeats
# For long-running tasks (GraphRAG), this determines recovery time after worker death
# Default: 300 seconds (5 minutes) - allows 10 missed heartbeats before declaring dead
STALE_TASK_TIMEOUT=300

# Max retries for failed tasks at each level
# After this many failures, task is marked as permanently failed
# Default: 3 retries
MAX_TASK_RETRIES=3

# DeepSeek OCR Service Configuration
DEEPSEEK_OCR_IP=localhost
DEEPSEEK_OCR_PORT=8005
DEEPSEEK_OCR_MODEL_NAME=deepseek-ai/DeepSeek-OCR
# Note: The model name will show as Qwen/Qwen3-0.6B even when DeepSeek-OCR is loaded
# This is expected because DeepSeek-OCR is built on top of Qwen3-0.6B base model
DEEPSEEK_OCR_TEMPERATURE=0.0
# Max tokens for output generation (model max_model_len is 8192, leave room for input)
# For large images with many crops, input can be 2000-4000 tokens
# Setting to 6144 leaves room for ~2000 input tokens
DEEPSEEK_OCR_MAX_TOKENS=6144
DEEPSEEK_OCR_NGRAM_SIZE=30
DEEPSEEK_OCR_WINDOW_SIZE=90
# Whitelist token IDs for <td> and </td> tags (comma-separated)
DEEPSEEK_OCR_WHITELIST_TOKEN_IDS=128821,128822


# ==============================================================================
# OLLAMA SERVICES CONFIGURATION
# ==============================================================================
# Two separate Ollama instances are used:
#   1. ollama-llm (port 11434)       -> LLM text inference for RAG agent
#   2. ollama-qwen3-vl (port 11435)  -> Vision-Language model for OCR
# ==============================================================================

# ------------------------------------------------------------------------------
# OCR SERVICE - Ollama Qwen3-VL (Docker: ollama-qwen3-vl, Port: 11435)
# Used for: Image OCR, document image analysis, vision tasks
# ------------------------------------------------------------------------------
OCR_OLLAMA_BASE_URL=http://127.0.0.1:11435
OCR_OLLAMA_MODEL=qwen3-vl:8b-instruct
OCR_OLLAMA_TEMPERATURE=0.5
OCR_OLLAMA_TIMEOUT=360
OCR_OLLAMA_MAX_TOKENS=40560

# Image Analysis Backend Selection (which backend to use for image OCR/analysis)
#   ollama       -> use Ollama server (OCR_OLLAMA_BASE_URL)
#   vllm         -> use vLLM OpenAI-compatible server (QWEN_VLLM_API_BASE)
#   transformers -> use HuggingFace transformers library (local GPU inference)
IMAGE_ANALYSIS_BACKEND=ollama

# OCR reasoning mode
#   thinking    -> reasoning model that emits <think>...</think>
#   instruction -> instruction-only model (no reasoning blocks)
QWEN_REASONING_MODE=instruction

# Image analysis backend for dots_ocr_service (gemma3 or qwen3)
IMAGE_ANALYSIS_BACKEND=qwen3

# Image Size Detection Threshold for OCR
# Simple images: area < threshold, Complex images: area >= threshold
QWEN_IMAGE_PIXEL_AREA_THRESHOLD=250000

# ------------------------------------------------------------------------------
# RAG AGENT LLM BACKEND SELECTION
# Used for: RAG query processing, text generation, agent responses
# Options: ollama, vllm
# ------------------------------------------------------------------------------
RAG_LLM_BACKEND=vllm

# ------------------------------------------------------------------------------
# RAG AGENT SERVICE - Ollama LLM (Docker: ollama-llm, Port: 11434)
# Active when RAG_LLM_BACKEND=ollama
# ------------------------------------------------------------------------------
RAG_OLLAMA_HOST=localhost
RAG_OLLAMA_PORT=11434
RAG_OLLAMA_MODEL=qwen2.5:latest
# Smaller, faster model for query analysis (query enhancement before vector search)
RAG_OLLAMA_QUERY_MODEL=qwen2.5:latest

# ------------------------------------------------------------------------------
# RAG AGENT SERVICE - vLLM (Docker: vllm_qwen3_14b, Port: 8004)
# Active when RAG_LLM_BACKEND=vllm
# Uses OpenAI-compatible API endpoint
# ------------------------------------------------------------------------------
RAG_VLLM_HOST=localhost
RAG_VLLM_PORT=8004
RAG_VLLM_MODEL=Qwen/Qwen3-8B
RAG_VLLM_API_KEY=EMPTY
# Reasoning mode for Qwen3 model:
#   instruction -> direct answers without reasoning blocks (default)
#   thinking    -> enable chain-of-thought reasoning with <think>...</think> blocks
RAG_VLLM_REASONING_MODE=instruction

# Auto-Recovery Configuration
# AUTO_RESUME_OCR: Resume incomplete OCR conversion tasks on startup (default: true)
# This will automatically resume OCR conversion for documents with pending/converting/partial/failed status
AUTO_RESUME_OCR=true
# AUTO_RESUME_INDEXING: Resume incomplete indexing tasks on startup (default: false)
# This will automatically resume indexing for documents with pending/failed status
# Set to false to prevent any automatic indexing on startup
AUTO_RESUME_INDEXING=true

# File Summary Configuration
# Enable/disable LLM-based file summary generation during indexing and query
# When disabled, file summaries will not be generated (saves LLM calls)
# and two-phase retrieval will skip file summary search
FILE_SUMMARY_ENABLED=false

# ------------------------------------------------------------------------------
# GEMMA3 IMAGE ANALYSIS (Legacy/Alternative - uses RAG Ollama server)
# ------------------------------------------------------------------------------
GEMMA_OLLAMA_BASE_URL=http://127.0.0.1:11434
GEMMA_MODEL=gemma3:27b
GEMMA_TEMPERATURE=0.8
GEMMA_TIMEOUT=120

# ------------------------------------------------------------------------------
# QWEN3 VIA VLLM (Alternative backend, if IMAGE_ANALYSIS_BACKEND=vllm)
# ------------------------------------------------------------------------------
# QWEN_VLLM_API_BASE=http://localhost:8006/v1
# QWEN_VLLM_MODEL=Qwen/Qwen2.5-VL-32B-Instruct-AWQ
# QWEN_VLLM_TEMPERATURE=0.8
# QWEN_VLLM_TIMEOUT=360
# QWEN_VLLM_MAX_TOKENS=20480

# ------------------------------------------------------------------------------
# QWEN3 VIA TRANSFORMERS (Alternative backend, if IMAGE_ANALYSIS_BACKEND=transformers)
# ------------------------------------------------------------------------------
QWEN_TRANSFORMERS_MODEL=Qwen/Qwen3-VL-8B-Instruct
QWEN_TRANSFORMERS_GPU_DEVICES=4,5,6
QWEN_TRANSFORMERS_DTYPE=bfloat16
QWEN_TRANSFORMERS_ATTN_IMPL=eager
QWEN_TRANSFORMERS_MAX_NEW_TOKENS=16384
QWEN_TRANSFORMERS_TOP_P=0.8
QWEN_TRANSFORMERS_TOP_K=20
QWEN_TRANSFORMERS_REPETITION_PENALTY=1.0
QWEN_TRANSFORMERS_DO_SAMPLE=false
QWEN_TRANSFORMERS_MAX_IMAGE_AREA=1000000

# Qdrant Vector Database Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Embedding Service Configuration
EMBEDDING_HOST=localhost
EMBEDDING_PORT=8003

# PostgreSQL Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=6400
POSTGRES_DB=dots_ocr
POSTGRES_USER=postgres
POSTGRES_PASSWORD=FyUbuntu@2025Ai

# JWT Authentication Configuration
JWT_SECRET_KEY=your-secret-key-change-in-production-to-random-string-abc123xyz789
# Set very long expiration so session only ends when user clicks logout
ACCESS_TOKEN_EXPIRE_MINUTES=525600
REFRESH_TOKEN_EXPIRE_DAYS=3650

# Neo4j Graph Database Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=securepassword123

# GraphRAG Configuration (Following Graph-R1 paper design)
# Enable/disable GraphRAG features (set to false to use traditional RAG only)
GRAPH_RAG_INDEX_ENABLED=true
GRAPH_RAG_QUERY_ENABLED=false
# Enable/disable embeddings for graph nodes and relationships
GRAPH_RAG_EMBEDDINGS_ENABLED=true
# Default query mode: auto, local, global, hybrid
# auto: LLM-based query mode detection (recommended)
# local: Entity-focused queries
# global: Relationship-focused queries
# hybrid: Combined entity and relationship queries (default)
GRAPH_RAG_DEFAULT_MODE=auto

# ========== Performance Optimization Settings ==========
# Maximum gleaning iterations for entity extraction
# 0 = No gleaning (fastest, 75% fewer LLM calls)
# 1 = One gleaning pass (balanced, 50% fewer LLM calls)
# 2-3 = Multiple passes (thorough but slow)
# Recommendation: 0 for speed, 1 for balanced quality/speed
GRAPH_RAG_MAX_GLEANING=0

# Minimum importance score for entity extraction (1-100)
# Only entities with score >= this threshold will be stored
# 80-100: Only critical entities (very selective)
# 60-79: Important entities (recommended - good balance)
# 40-59: Moderate entities (more comprehensive)
# Below 40: All entities (not recommended - too noisy)
GRAPH_RAG_MIN_ENTITY_SCORE=40

# Entity extraction batch size
GRAPH_RAG_ENTITY_BATCH_SIZE=10
# Top-k results for graph retrieval
GRAPH_RAG_TOP_K=60
# Enable/disable Qdrant vector search (always combines with graph results)
GRAPH_RAG_VECTOR_SEARCH_ENABLED=true
# Iterative reasoning max steps (Graph-R1 think-query-retrieve-rethink cycle)
# This sets the MAXIMUM allowed steps. The LLM will analyze query complexity
# and determine the actual steps needed (1 to GRAPH_RAG_MAX_STEPS).
# Set to 1 for single-step retrieval only
# Set to 3-5 for allowing iterative reasoning on complex queries (recommended: 3)
GRAPH_RAG_MAX_STEPS=3

# Number of chunks to show per source document in GraphRAG reasoning prompts
# Higher values provide more context but increase token usage
# Recommendation: 2-3 chunks per source for balanced coverage
GRAPH_RAG_CHUNKS_PER_SOURCE=3

# Graph-R1 Reward-Based Termination (Quality Control)
# Enable/disable reward scoring for answer quality assessment
GRAPH_RAG_ENABLE_REWARD_SCORING=true
# Use LLM-based answer scoring (true) or heuristic-based (false)
# LLM scoring is more accurate but slower; heuristic is faster but less intelligent
GRAPH_RAG_USE_LLM_ANSWER_SCORING=true
# High format score threshold (0.0-1.0) - terminate if answer is well-formatted and high quality
GRAPH_RAG_FORMAT_REWARD_HIGH_THRESHOLD=0.8
# Low format score threshold (0.0-1.0) - terminate if likely hallucination
GRAPH_RAG_FORMAT_REWARD_LOW_THRESHOLD=0.3
# Minimum answer quality score (0.0-1.0) - continue if answer quality is below this
GRAPH_RAG_ANSWER_REWARD_THRESHOLD=0.6

# ==============================================================================
# ITERATIVE REASONING CONFIGURATION
# ==============================================================================
# Enable/disable iterative reasoning for vector-only search (works with or without GraphRAG)
# When enabled, queries are refined iteratively based on retrieved results
ITERATIVE_REASONING_ENABLED=true
# Default max steps for iterative reasoning (can be overridden by LLM complexity analysis)
ITERATIVE_REASONING_DEFAULT_MAX_STEPS=3
# Number of chunks to show per source in the reasoning prompt
ITERATIVE_REASONING_CHUNKS_PER_SOURCE=3

# ==============================================================================
# ADAPTIVE CHUNKING CONFIGURATION
# ==============================================================================
# Enable/disable adaptive domain-aware chunking (default: true for optimal chunking)
# When enabled, documents are classified before chunking to select optimal strategy
# This provides domain-aware chunking with rich metadata for better retrieval
ADAPTIVE_CHUNKING_ENABLED=true
# Enable LLM-based document classification (more accurate but slower)
# When disabled (recommended), uses fast rule-based classification only
# Rule-based classification is sufficient for most use cases and avoids LLM latency/cost
ADAPTIVE_CHUNKING_USE_LLM=false

# Default chunk sizes per domain (used when adaptive chunking is enabled)
# Legal documents: contracts, agreements, terms of service
CHUNKING_LEGAL_SIZE=1024
CHUNKING_LEGAL_OVERLAP_PERCENT=5
# Academic documents: research papers, theses, articles
CHUNKING_ACADEMIC_SIZE=768
CHUNKING_ACADEMIC_OVERLAP_PERCENT=15
# Medical documents: clinical notes, records, reports
CHUNKING_MEDICAL_SIZE=1024
CHUNKING_MEDICAL_OVERLAP_PERCENT=5
# Engineering documents: specs, requirements, API docs
CHUNKING_ENGINEERING_SIZE=1024
CHUNKING_ENGINEERING_OVERLAP_PERCENT=5
# Education documents: textbooks, course materials, exams
CHUNKING_EDUCATION_SIZE=768
CHUNKING_EDUCATION_OVERLAP_PERCENT=10
# Financial documents: receipts, invoices, statements
CHUNKING_FINANCIAL_SIZE=1024
CHUNKING_FINANCIAL_OVERLAP_PERCENT=15
# General documents: default for unclassified
CHUNKING_GENERAL_SIZE=512
CHUNKING_GENERAL_OVERLAP_PERCENT=10

# Size thresholds for document classification
# Documents below this token count may be kept whole
CHUNKING_SMALL_DOC_TOKENS=1000
# Documents above this token count use more aggressive splitting
CHUNKING_LARGE_DOC_TOKENS=10000

# ==============================================================================
# PARENT/CHILD CHUNK CONFIGURATION
# ==============================================================================
# Enable parent/child chunk design for hierarchical context retrieval
# When enabled, large sections create both a parent chunk (full section)
# and child chunks (split pieces), with bidirectional references
PARENT_CHUNK_ENABLED=true
# Automatically include parent chunks when child chunks match a query
# This provides expanded context for better understanding
# When true: Parent chunks are always fetched for child matches
# When false: Only child chunks are returned (caller must fetch parents manually)
PARENT_CHUNK_AUTO_INCLUDE=true

# ==============================================================================
# CHAT CONTEXT MANAGEMENT CONFIGURATION
# ==============================================================================
# Chat Context Configuration for short-term memory and context awareness
# Number of messages to keep in short-term memory (conversation context window)
CHAT_CONTEXT_WINDOW_SIZE=20
# Maximum input token limit for chat history passed to LLM
# When chat history exceeds this limit, only the most recent messages are kept
# Default: 4096 tokens (adjust based on your model's context window)
MAX_INPUT_TOKEN_LIMIT=4096
# Enable/disable context analysis (pronoun detection, entity extraction, reference resolution)
CHAT_ENABLE_CONTEXT_ANALYSIS=true
# Enable/disable metadata tracking (track entities, topics, documents in session metadata)
CHAT_ENABLE_METADATA_TRACKING=true
# Maximum number of entities to track per type (documents, people, topics, etc.)
CHAT_MAX_ENTITIES_PER_TYPE=10
# Maximum number of topics to track in conversation
CHAT_MAX_TOPICS=5
# Enable automatic fallback from CONVERSATION_ONLY to document search
# When enabled, if LLM response from history indicates "no data found",
# the system will automatically trigger a document search with enhanced query
CHAT_AUTO_FALLBACK_ENABLED=true

# ==============================================================================
# DOCUMENT METADATA VECTOR ROUTING CONFIGURATION
# ==============================================================================
# Enable/disable vector-based document routing (faster than LLM scoring)
# When enabled, queries are routed to relevant documents using semantic similarity
# on document metadata embeddings stored in Qdrant
METADATA_VECTOR_ROUTING_ENABLED=true

# Number of top documents to return from metadata vector search
METADATA_SEARCH_TOP_K=15

# Minimum similarity score threshold for metadata vector search (0.0-1.0)
# Lower values = more results (better recall), higher values = fewer but more relevant results
METADATA_SCORE_THRESHOLD=0.4

# Minimum number of results from vector search before falling back to LLM scoring
# If vector search returns fewer results than this, LLM fallback is NOT triggered
# (vector results are still used if any are found)
METADATA_FALLBACK_MIN_RESULTS=3

# Adaptive relative threshold for document routing (0.0-1.0)
# Results must score >= (top_score * this_value) to be included
# Higher = stricter filtering (fewer documents), Lower = more permissive
# Example: 0.5 means results must be at least 50% of the top score
METADATA_RELATIVE_THRESHOLD=0.7

# Entity matching boost for document routing
# Boosts score when query entities match document source/content
ENTITY_MATCH_BOOST=0.3
