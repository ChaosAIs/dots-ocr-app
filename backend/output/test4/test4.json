[{"bbox": [132, 29, 179, 44], "category": "Page-header", "text": "Preprint."}, {"bbox": [134, 101, 620, 238], "category": "Picture"}, {"bbox": [132, 247, 627, 278], "category": "Caption", "text": "Figure 4: Step-wise F1 score on HotpotQA based on Qwen2.5 (1.5B, 3B, 7B), where Graph-R1 out-\nperforms baselines and GPT-4o-mini variants (NaiveGeneration, StandardRAG, HyperGraphRAG)."}, {"bbox": [132, 294, 627, 328], "category": "Text", "text": "where each $\\tau_i = (\\langle s_i^{(i)}, \\mathbf{a}_t^{(i)} \\rangle, \\dots, \\langle s_T^{(i)}, \\mathbf{a}_T^{(i)} \\rangle)$ denotes a sequence of state-action pairs sampled from the environment. We optimize the policy $\\pi_{\\theta}$ using the GRPO-based objective, which is defined as:"}, {"bbox": [145, 329, 627, 442], "category": "Formula", "text": "$$\n\\begin{align}\n\\mathcal{J}_{\\text{GRPO}}(\\theta) &= \\mathbb{E}_{\\{s_t \\sim \\{\\langle q \\rangle | q \\in D_Q\\}, \\{\\tau_i\\}_{i=1}^N \\sim \\pi_{\\theta_{\\text{act}}} (\\tau_v | s_t; \\mathcal{G}_H)\\}} \\nonumber \\\\\n&\\quad \\left[ \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\tau_i} \\sum_{t=1}^{\\tau_i} \\min \\left( \\rho_{\\theta}(\\mathbf{a}_t^{(i)}) \\hat{A}(\\tau_i), \\text{clip}(\\rho_{\\theta}(\\mathbf{a}_t^{(i)}), 1 \\pm \\epsilon) \\hat{A}(\\tau_i) \\right) - \\beta \\mathbb{D}_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) \\right], \\tag{11} \\\\\n\\text{where } \\rho_{\\theta}(\\mathbf{a}_t^{(i)}) &= \\frac{\\pi_{\\theta}(\\mathbf{a}_t^{(i)} | s_{t-1}^{(i)}; \\mathcal{G}_H)}{\\pi_{\\theta_{\\text{act}}}(\\mathbf{a}_t^{(i)} | s_{t-1}^{(i)}; \\mathcal{G}_H)}, \\quad \\text{and } \\hat{A}(\\tau_i) &= \\frac{R(\\tau_i) - \\text{mean}((R(\\tau_j))_{j=1}^N)}{F_{\\text{norm}}((R(\\tau_i))_{j=1}^N)}. \\tag{12}\n\\end{align}\n$$"}, {"bbox": [132, 440, 627, 515], "category": "Text", "text": "Here, $\\pi_{\\theta}$ is the current policy, and $\\pi_{\\theta_{\\text{act}}}$ is the behavior policy used for sampling. The importance ratio $\\rho_{\\theta}(\\mathbf{a}_t^{(i)})$ adjusts for distribution shift, while the advantage $\\hat{A}(\\tau_i)$ normalizes the reward using a scaling function $F_{\\text{norm}}(\\cdot)$ (e.g., standard deviation). The clip(·) operator stabilizes updates by constraining policy shifts. A KL term $D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}})$ regularizes toward a reference policy $\\pi_{\\text{ref}}$, with $\\beta$ controlling its strength. This objective encourages high-reward, stable reasoning over $\\mathcal{G}_H$."}, {"bbox": [132, 518, 627, 563], "category": "Text", "text": "**Outcome-directed Reward Function** $R(\\tau)$. To meet outcome requirements, we define a re-\nward function $R(\\tau)$ composed of two parts: a *format reward* $R_{\\text{format}}(\\tau)$ and an *answer reward*\n$R_{\\text{answer}}(\\mathbf{a}_T^{\\text{ans}})$, promoting both thoughtful retrieval and accurate answer generation."}, {"bbox": [132, 566, 627, 610], "category": "List-item", "text": "(i) **Format Reward.** The format reward $R_{\\text{format}}(\\tau)$ encourages the agent to follow the intended reasoning structure. At each step $(s_t, \\alpha_t)$, we check whether the output includes a well-formed block $(\\mathbf{a}_t^{\\text{link}}, \\alpha_t, \\alpha_t^{\\text{out}})$. Each valid step receives 0.5 reward, capped at 1.0 overall:"}, {"bbox": [198, 611, 627, 656], "category": "Formula", "text": "$$\nR_{\\text{format}}(\\tau) = \\min \\left( 1.0, 0.5 \\cdot \\sum_{t=1}^{T} \\left\\{ (\\mathbf{a}_t^{\\text{link}}, \\alpha_t, \\alpha_t^{\\text{out}}) \\text{ is well-formed} \\right\\} \\right), \\quad (13)\n$$"}, {"bbox": [132, 656, 607, 672], "category": "Text", "text": "where $\\mathbb{I}\\{\\cdot\\}$ is an indicator function that returns 1 if the step output matches the expected format."}, {"bbox": [132, 675, 627, 707], "category": "List-item", "text": "(ii) **Answer Reward.** The answer reward $R_{\\text{answer}}(\\mathbf{a}_T^{\\text{ans}})$ measures the semantic correctness of the generated answer $\\mathbf{a}_T^{\\text{ans}}$ by comparing it with the ground-truth answer $y_v^*$ using a token-level F1 score:"}, {"bbox": [260, 707, 627, 738], "category": "Formula", "text": "$$\nR_{\\text{answer}}(\\mathbf{a}_T^{\\text{ans}}) = \\frac{2 \\cdot |\\text{tokens}(\\mathbf{a}_T^{\\text{ans}}) \\cap \\text{tokens}(y_v^*)|}{|\\text{tokens}(\\mathbf{a}_T^{\\text{ans}})| + |\\text{tokens}(y_v^*)|}, \\quad (14)\n$$"}, {"bbox": [132, 744, 627, 774], "category": "Text", "text": "where $|\\cdot|$ denotes multiset cardinality. The function tokens(·) applies standard preprocessing in-\ncluding lowercasing and whitespace-based tokenization."}, {"bbox": [132, 778, 627, 857], "category": "List-item", "text": "(iii) **Overall Outcome Reward.** The total reward for a reasoning trajectory $\\tau$ is defined as:\n$$R(\\tau) = 1.0 + R_{\\text{format}}(\\tau) + \\mathbb{I}\\{R_{\\text{format}}(\\tau) = 1.0\\} \\cdot R_{\\text{answer}}(\\mathbf{a}_T^{\\text{ans}}), \\text{ where } \\mathbf{a}_T^{\\text{ans}} \\in \\tau, \\quad (15)$$\nensuring that answer correctness is only rewarded when the format is structurally valid. With the outcome-directed reward $R(\\tau)$, high answer quality $\\mathbf{a}_T^{\\text{ans}}$ is attainable through structurally coherent and reasoning-complete trajectories $\\tau$ with multi-turn iteration with knowledge hypergraph $\\mathcal{G}_H$."}, {"bbox": [132, 861, 602, 877], "category": "Text", "text": "**Proposition 3.** *End-to-end RL bridges the gap between graph-based knowledge and language.*"}, {"bbox": [132, 879, 627, 896], "category": "Text", "text": "*Proof.* We provide experimental results in Section 5.6 and theoretical proofs in Appendix B.3 □"}]