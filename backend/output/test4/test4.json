[{"bbox": [132, 29, 179, 45], "category": "Page-header", "text": "Preprint."}, {"bbox": [135, 101, 620, 238], "category": "Picture"}, {"bbox": [132, 248, 627, 279], "category": "Caption", "text": "Figure 4: Step-wise F1 score on HotpotQA based on Qwen2.5 (1.5B, 3B, 7B), where Graph-R1 out-\nperforms baselines and GPT-4o-mini variants (NaiveGeneration, StandardRAG, HyperGraphRAG)."}, {"bbox": [132, 294, 627, 328], "category": "Text", "text": "where each $\\tau_i = (\\langle s_i^{(i)}, a_i^{(i)} \\rangle, \\dots, \\langle s_T^{(i)}, a_T^{(i)} \\rangle)$ denotes a sequence of state-action pairs sampled from the environment. We optimize the policy $\\pi_{\\theta}$ using the GRPO-based objective, which is defined as:"}, {"bbox": [145, 329, 627, 442], "category": "Formula", "text": "$$\n\\begin{align}\n\\mathcal{J}_{\\text{GRPO}}(\\theta) &= \\mathbb{E}_{\\{s_i \\sim \\{\\langle q \\rangle | i \\in D_Q\\}, \\{\\tau_i\\}_{i=1}^N \\sim \\pi_{\\theta_{\\text{act}}} (\\tau_v | s_i; \\hat{G}_H)\\}} \\nonumber \\\\\n&\\quad \\left[ \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\tau_i} \\sum_{t=1}^{\\tau_i} \\min \\left( \\rho_{\\theta}(\\mathbf{a}_t^{(i)}) \\hat{A}(\\tau_i), \\text{clip}(\\rho_{\\theta}(\\mathbf{a}_t^{(i)}), 1 \\pm \\epsilon) \\hat{A}(\\tau_i) \\right) - \\beta \\mathbb{D}_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) \\right], \\tag{11} \\\\\n\\text{where } \\rho_{\\theta}(\\mathbf{a}_t^{(i)}) &= \\frac{\\pi_{\\theta}(\\mathbf{a}_t^{(i)} | s_{t-1}^{(i)}; \\hat{G}_H)}{\\pi_{\\theta_{\\text{act}}}(\\mathbf{a}_t^{(i)} | s_{t-1}^{(i)}; \\hat{G}_H)}, \\quad \\text{and } \\hat{A}(\\tau_i) &= \\frac{R(\\tau_i) - \\text{mean}((R(\\tau_j))_{j=1}^N)}{F_{\\text{norm}}((R(\\tau_i))_{j=1}^N)}. \\tag{12}\n\\end{align}\n$$"}, {"bbox": [132, 440, 627, 516], "category": "Text", "text": "Here, $\\pi_{\\theta}$ is the current policy, and $\\pi_{\\theta_{\\text{act}}}$ is the behavior policy used for sampling. The importance ratio $\\rho_{\\theta}(\\mathbf{a}_t^{(i)})$ adjusts for distribution shift, while the advantage $\\hat{A}(\\tau_i)$ normalizes the reward using a scaling function $F_{\\text{norm}}(\\cdot)$ (e.g., standard deviation). The clip(·) operator stabilizes updates by constraining policy shifts. A KL term $D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}})$ regularizes toward a reference policy $\\pi_{\\text{ref}}$, with $\\beta$ controlling its strength. This objective encourages high-reward, stable reasoning over $\\hat{G}_H$."}, {"bbox": [132, 519, 627, 564], "category": "Text", "text": "**Outcome-directed Reward Function** $R(\\tau)$. To meet outcome requirements, we define a re-\nward function $R(\\tau)$ composed of two parts: a *format reward* $R_{\\text{format}}(\\tau)$ and an *answer reward*\n$R_{\\text{answer}}(\\mathbf{a}_T^{\\text{ans}})$, promoting both thoughtful retrieval and accurate answer generation."}, {"bbox": [132, 566, 627, 610], "category": "List-item", "text": "(i) **Format Reward.** The format reward $R_{\\text{format}}(\\tau)$ encourages the agent to follow the intended reasoning structure. At each step $(s_t, \\alpha_t)$, we check whether the output includes a well-formed block $(\\mathbf{a}_t^{\\text{link}}, \\alpha_t, \\alpha_t^{\\text{out}})$. Each valid step receives 0.5 reward, capped at 1.0 overall:"}, {"bbox": [199, 611, 627, 656], "category": "Formula", "text": "$$\nR_{\\text{format}}(\\tau) = \\min \\left( 1.0, 0.5 \\cdot \\sum_{t=1}^{T} \\left\\{ (\\mathbf{a}_t^{\\text{link}}, \\alpha_t, \\alpha_t^{\\text{out}}) \\text{ is well-formed} \\right\\} \\right), \\quad (13)\n$$"}, {"bbox": [132, 655, 608, 672], "category": "Text", "text": "where $\\mathbb{I}\\{\\cdot\\}$ is an indicator function that returns 1 if the step output matches the expected format."}, {"bbox": [132, 674, 627, 707], "category": "List-item", "text": "(ii) **Answer Reward.** The answer reward $R_{\\text{answer}}(\\mathbf{a}_T^{\\text{ans}})$ measures the semantic correctness of the generated answer $\\mathbf{a}_T^{\\text{ans}}$ by comparing it with the ground-truth answer $y_T^*$ using a token-level F1 score:"}, {"bbox": [260, 707, 627, 738], "category": "Formula", "text": "$$\nR_{\\text{answer}}(\\mathbf{a}_T^{\\text{ans}}) = \\frac{2 \\cdot |\\text{tokens}(\\mathbf{a}_T^{\\text{ans}}) \\cap \\text{tokens}(y_T^*)|}{|\\text{tokens}(\\mathbf{a}_T^{\\text{ans}})| + |\\text{tokens}(y_T^*)|}, \\quad (14)\n$$"}, {"bbox": [132, 743, 627, 774], "category": "Text", "text": "where $|\\cdot|$ denotes multiset cardinality. The function tokens(·) applies standard preprocessing in-\ncluding lowercasing and whitespace-based tokenization."}, {"bbox": [132, 777, 577, 795], "category": "List-item", "text": "(iii) Overall Outcome Reward. The total reward for a reasoning trajectory $\\tau$ is defined as:"}, {"bbox": [175, 794, 627, 814], "category": "Formula", "text": "$$\nR(\\tau) = 1.0 + R_{\\text{format}}(\\tau) + \\mathbb{I}\\{R_{\\text{format}}(\\tau) = 1.0\\} \\cdot R_{\\text{answer}}(\\mathbf{a}_T^{\\text{ans}}), \\text{ where } \\mathbf{a}_T^{\\text{ans}} \\in \\tau, \\quad (15)\n$$"}, {"bbox": [132, 813, 627, 857], "category": "Text", "text": "ensuring that answer correctness is only rewarded when the format is structurally valid. With the outcome-directed reward $R(\\tau)$, high answer quality $\\mathbf{a}_T^{\\text{ans}}$ is attainable through structurally coherent and reasoning-complete trajectories $\\tau$ with multi-turn iteration with knowledge hypergraph $\\hat{G}_H$."}, {"bbox": [132, 861, 602, 877], "category": "Text", "text": "**Proposition 3.** *End-to-end RL bridges the gap between graph-based knowledge and language.*"}, {"bbox": [132, 879, 627, 896], "category": "Text", "text": "*Proof.* We provide experimental results in Section 5.6 and theoretical proofs in Appendix B.3 □"}]