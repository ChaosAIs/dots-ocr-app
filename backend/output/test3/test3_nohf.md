You are a helpful assistant. Answer the given question. You can query from knowledge base provided to you to answer the question. You can query knowledge as many times as you want. You must first conduct reasoning inside `<think>...</think>`. If you need to query knowledge, you can set a query statement between `<query>...</query>` to query from knowledge base after `<think>...</think>`. When you have the final answer, you can output the answer inside `<answer>...</answer>`. Question: question. Assistant:

Table 1: Template for Graph-R1. question will be replaced with the specific user query. Note that the knowledge retrieved is placed within `<knowledge>...</knowledge>` after `</query>`.

**Knowledge Interaction via Hypergraph Retrieval.** Given a query $\mathbf{a}_t^{\text{query}}$ generated by the reasoning LLM, we retrieve relevant knowledge $\mathbf{a}_t^k$ from the hypergraph $\mathcal{G}_H = (V, E_H)$ through a dual-path interaction process: entity-based retrieval and direct hyperedge retrieval. The resulting n-ary relational facts are then aggregated via rank-based fusion to support downstream reasoning.

(i) **Entity-based Hyperedge Retrieval.** We first identify a set of top-ranked entities based on their similarity to the extracted entities $V_{\mathbf{a}_t^{\text{query}}}$, and collect hyperedges that connect to any retrieved entity:

$$
\mathcal{R}_V(\mathbf{a}_t^{\text{query}}) = \underset{v \in V}{\operatorname{argmax}} \left( \operatorname{sim}(\phi(V_{\mathbf{a}_t^{\text{query}}}), \phi(v)) \right), \quad \mathcal{F}_V^k = \bigcup_{v_i \in \mathcal{R}_V} \{ (e_H, V_{e_H}) \mid v_i \in V_{e_H}, e_H \in E_H \}, \quad (7)
$$

where $\phi(V_{\mathbf{a}_t^{\text{query}}})$ is the aggregated embedding of entities extracted from $\mathbf{a}_t^{\text{query}}$, $\phi(v)$ is the entity embedding, $k_V$ is the number of retrieved entities, and $V_{e_H}$ denotes the entity set of hyperedge $e_H$.

(ii) **Direct Hyperedge Retrieval.** In parallel, we directly retrieve hyperedges based on query-hyperedge similarity, and collect their associated relational facts:

$$
\mathcal{R}_H(\mathbf{a}_t^{\text{query}}) = \underset{e_H \in E_H}{\operatorname{argmax}} \left( \operatorname{sim}(\phi(\mathbf{a}_t^{\text{query}}), \phi(e_H)) \right), \quad \mathcal{F}_H^k = \bigcup_{e_i \in \mathcal{R}_H} \{ (e_i, V_{e_i}) \mid V_{e_i} \subseteq V \}, \quad (8)
$$

where $\phi(\mathbf{a}_t^{\text{query}})$ is the query embedding, $\phi(e_H)$ is the hyperedge embedding, $k_H$ is the number of retrieved hyperedges, and $V_{e_i}$ denotes the entity set of hyperedge $e_i$.

(iii) **Fusion via Reciprocal Rank Aggregation.** To produce the final knowledge set, we merge results from both retrieval paths using reciprocal rank aggregation over hyperedges:

$$
\mathbf{a}_t^{\text{rel}} = \mathcal{F}_t^{\text{query}} = \text{Top-k} \left( \mathcal{F}_V^k \cup \mathcal{F}_H^*, \text{RankScore}(f) \right), \quad \text{RankScore}(f) = \frac{1}{r_V} + \frac{1}{r_H} \mathbf{a}_t^{\text{query}}, \quad (9)
$$

where $r_V$ and $r_H$ are the ranks of n-ary relational fact $f$ in $\mathcal{F}_V^k$ and $\mathcal{F}_H^*$ respectively (set to $\infty$ if absent), and $k$ is the number of retrieved facts $\mathbf{a}_t^{\text{rel}}$ returned to the agent.

**Optimization Objective for Agent Trajectories.** The agent aims to learn a reasoning trajectory $\tau \in \mathcal{T}_q$ that yields a faithful and contextually grounded answer $y_q$. Each trajectory $\tau = ((s_1, a_1), (s_2, a_2), \dots, (s_T, a_T))$ comprises a sequence of actions executed over $\hat{\mathcal{G}}_H$, defined as:

$$
\max_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}} (\tau_q | q; \hat{\mathcal{G}}_H) [\log P(y_q | \tau)], \quad (10)
$$

where $P(y_q | \tau)$ denotes the likelihood of the correct answer $y_q \sim \mathbf{a}_t^{\text{ans}}$ under trajectory $\tau$, guiding $\pi_\theta$ toward answer-consistent reasoning.

**Proposition 2.** *Multi-turn interaction with the graph environment improves retrieval efficiency.*

*Proof.* We provide experimental results in Section 5.5 and theoretical proofs in Appendix B.2. â–¡

## 4.3 OUTCOME-DIRECTED END-TO-END REINFORCEMENT LEARNING

To optimize the reasoning policy $\pi_{\theta}$ toward generating faithful and well-structured answers, we adopt an end-to-end reinforcement learning objective based on Group Relative Policy Optimization (GRPO) (Shao et al., 2024) $\mathcal{J}_{\text{GRPO}}(\theta)$ and design an outcome-directed reward function $R(\tau)$.

**End-to-end RL Objective** $\mathcal{J}_{\text{GRPO}}(\theta)$. Given a dataset question $q \in \mathcal{D}_Q$, the agent interacts with the knowledge hypergraph $\hat{\mathcal{G}}_H$ to generate a group of multi-turn reasoning trajectories $\{\tau_i\}_{i=1}^N \subseteq \mathcal{T}_q$.